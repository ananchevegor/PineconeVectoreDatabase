# Информация, относящаяся к теме
Во-первых, это все для меня. Тут будут просто записки того что я понял и тд и тп.

```

1. Векторные базы данных хранят информацию в векторах и осуществляют поиск "по смыслу"(семастический поиск). Это значит, что в отличие от обычной индексированной базы данных, где осуществляется поиск по id(или другим параметрам), который должен СТРОГО совпадать с записью, в векторной это совпадение не обязательное условие.

2. Поиск осуществляется по косинусной метрике(косинусу между векторами). Это можно представить в виде коминуса угла между двумя гипотинузами, образованными расстоянием между тремя вершинами графа(начального вектора[тот что мы задали], и двух его близжайших соседей). Чем ближе косинус к 1, тем больше совпадения между векторами

3. Почему вектора? они имеют направление! Вектор в два раза больший по длине будет иметь тот же косинус, что и вектор в два раза меньший. Вектор в противоположном направлении будет иметь косинус -1

```

# Embedding

Embedding - алгоритм преобразования данных в вектор.

### Типы Embedding

  ```

   1. Danse - вектор имеющий ненулевые значения элементов. Эти вектора обычно имеют меньшую размерность. Хорошо подходят для поисков.

   2. Spaese - вектор имеюющщий нулевые значения элементов. Много большая размерность(10к-1000кк). Не требуется для обычения для нейросетей. Используется в поиске в браузере

  ```

# Что делать, если данные большие. 

Предположим, что нам надо загрузить в базу "Войну и мир" Толстого. Мы не можем просто взять и загрузить одним скопом все, как минимум потому что это потеряет смысл, если мы будем искать по одной вершине из ЦЕЛОЙ книги.
Например при вводе "Что за дуб был в романе?" и "Как умер книзь Балконский?" будет ответ всей книгой. Нам это не нужно для дальшей обработки данных AI.

Мы должны разбить текст на чанки. Чанк - это данные определнной размерности.

Выделяется разное разбиение
  1. По смыслу(Семантический чанк)
  2. По количеству слов(например 500 слов)
  3. По главам(text.split("/n/n"))
  4. Кастомно по любому другому признаку

Так на два запроса будет выдано две близжайшие вершины графа. Они не будут полной книгой. Это будут отдельные предложения или абзацы.



# Что если данные это не текст

Предположим, что мы хотим занести в базу вектор файла(изображение, аудиофайл, ...)

Так как мы преобразовываем в вектор текст, то мы как бы сохраняем uuid, chunk_text, vector - по сути мы по вектору ищем chunk_text - он не имеет кодировки, обычный текст.

С файлами мы сохраняем uuid, file_link, vector - то же самое, но только мы должны хранить еще где то сами файлы. Обратно преобразовать из вектора в данные мы не можем.




# Построение индекса в базе данных

## Как это работает. Как работает поиск. Алгоритмы.

  HNSW(Hierarchical Navigable Small World) - алгоритм поиска на основе иерархии(слоев) с графами приближающимися в вершине-вектору на последнем слое
     
  Представление
  ![image](https://github.com/user-attachments/assets/9d0faf4c-6d5e-486c-b925-59b3aa96a819)


